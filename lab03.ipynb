{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo942LMOdlh4"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DokFOdD1dJEl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init(project=\"aml-lab03\", entity=\"peiro98\", name=\"aml-lab03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIDLJuIXK_vh"
   },
   "source": [
    "**Set Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d5PkYfqfK_SA"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 'cuda' or 'cpu'\n",
    "\n",
    "# available classes: \"dog\", \"elephant\", \"giraffe\", \"guitar\", \"horse\", \"house\", \"person\"\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "LR = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-5\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "LOG_FREQUENCY = 10\n",
    "TRAIN_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gwii0TBHvzh"
   },
   "source": [
    "**Define Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QUDdw4j2H0Mc"
   },
   "outputs": [],
   "source": [
    "# Define transforms for training phase\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        # 227x227 -> 224x224\n",
    "        transforms.CenterCrop(224),\n",
    "        # convert to tensor\n",
    "        transforms.ToTensor(),\n",
    "        # normalizes tensor with mean and standard deviation\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define transforms for the evaluation phase\n",
    "eval_transform = train_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qYIHPzYLY7i"
   },
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "QfVq_uDHLbsR",
    "outputId": "af2136aa-db20-43e9-8d77-502a323b9484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SRC] train size: 1670\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"PACS/data\"\n",
    "SRC_DATA_DIR = f\"{DATA_DIR}/photo/\"\n",
    "\n",
    "from PACS.PACS import PACSDataset\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "src_dataset = PACSDataset(SRC_DATA_DIR, transform=train_transform)\n",
    "\n",
    "print(\"[SRC] train size:\", len(src_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET] train size: 1536\n",
      "[TARGET] validation size: 994\n"
     ]
    }
   ],
   "source": [
    "TARGET_DATA_DIR = f\"{DATA_DIR}/art_painting/\"\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "target_dataset = PACSDataset(TARGET_DATA_DIR, transform=train_transform)\n",
    "\n",
    "# take the indicies corresponding to train samples\n",
    "target_train_indices = np.random.choice(\n",
    "    len(target_dataset), size=int(TRAIN_RATIO * len(target_dataset))\n",
    ")\n",
    "target_val_indices = np.setdiff1d(np.arange(len(target_dataset)), target_train_indices)\n",
    "\n",
    "target_train_dataset = Subset(target_dataset, target_train_indices)\n",
    "target_validation_dataset = Subset(target_dataset, target_val_indices)\n",
    "\n",
    "print(\"[TARGET] train size:\", len(target_train_dataset))\n",
    "print(\"[TARGET] validation size:\", len(target_validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYEDQ7Z21ldN"
   },
   "source": [
    "**Prepare Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VriRw8SI1nle"
   },
   "outputs": [],
   "source": [
    "# Train dataloaders\n",
    "src_dataloader = DataLoader(\n",
    "    src_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n",
    ")\n",
    "# src_train_dataloader = DataLoader(src_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "# target_train_dataloader = DataLoader(target_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# Validation dataloaders\n",
    "target_dataloader = DataLoader(\n",
    "    target_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True\n",
    ")\n",
    "# src_validation_dataloader = DataLoader(src_validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)\n",
    "# target_validation_dataloader = DataLoader(target_validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbZ1t5Qs2z4j"
   },
   "source": [
    "**Prepare Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "exHUjtXa22DN"
   },
   "outputs": [],
   "source": [
    "from DANN import build_model as build_DANN\n",
    "\n",
    "DANN = build_DANN(7, 2, True)  # n. of classes, n. of domains, pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEyL3H_R4qCf"
   },
   "source": [
    "**Prepare Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9sjq00G94tSc"
   },
   "outputs": [],
   "source": [
    "# classification => cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    DANN.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxYUli9d9uYQ"
   },
   "source": [
    "## Train w/o domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataloader):\n",
    "    DANN.train(False)\n",
    "\n",
    "    # predictions_table = wandb.Table(columns=[\"image\", \"label\", \"preds\"])\n",
    "\n",
    "    running_corrects = 0\n",
    "    for data, labels in dataloader:\n",
    "        data = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = DANN(data, True) # True for classifier\n",
    "\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update Corrects\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "        # if i == 0:\n",
    "        #     for image, label, pred in zip(images, labels, preds):\n",
    "        #         predictions_table.add_data(wandb.Image(image), label, pred)\n",
    "                \n",
    "    return running_corrects / float(len(target_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_loss(dataloader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    DANN.train(False)\n",
    "    \n",
    "    loss, n = 0, 0\n",
    "    for data, labels in dataloader:\n",
    "        data = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = DANN(data, True) # True for classifier\n",
    "\n",
    "        # compute the loss\n",
    "        loss += criterion(outputs.detach(), labels)\n",
    "        n = n + len(labels)\n",
    "       \n",
    "    return loss / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcoQ5fD49yT_"
   },
   "outputs": [],
   "source": [
    "DANN = DANN.to(DEVICE)  # this will bring the network to GPU if DEVICE is cuda\n",
    "# wandb.watch(DANN, log_freq=10)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # iterate over the dataset\n",
    "    for images, labels in src_dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        DANN.train()\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        outputs = DANN(images, True)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # compute and propagate the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    source_loss = epoch_loss / len(src_dataloader)\n",
    "    print(f\"[SOURCE]: (avg) loss is {source_loss:.3f}\")\n",
    "\n",
    "    target_loss = compute_classification_loss(target_dataloader)\n",
    "    target_accuracy = compute_accuracy(target_dataloader)\n",
    "    print(f\"[TARGET]: loss is {target_loss:.3f}, accuracy is {target_accuracy:.3f}\")\n",
    "    \n",
    "    # compute the average epoch loss\n",
    "    # wandb.log({\"training-loss\": training_loss})\n",
    "\n",
    "    # at the end of each epoch compute the accuracy on the validation accuracy\n",
    "    # wandb.log({\"validation-accuracy\": validation_loss})\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train w/ domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 1.96 GiB total capacity; 1023.27 MiB already allocated; 58.88 MiB free; 1.15 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23742/4180339393.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# compute and propagate the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcurrent_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py-venv/lib64/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py-venv/lib64/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py-venv/lib64/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py-venv/lib64/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             F.sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    111\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py-venv/lib64/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 1.96 GiB total capacity; 1023.27 MiB already allocated; 58.88 MiB free; 1.15 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "DANN = DANN.to(DEVICE)  # this will bring the network to GPU if DEVICE is cuda\n",
    "# wandb.watch(DANN, log_freq=10)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "domain_classifier_criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    DANN.train(True)\n",
    "\n",
    "    # iterate over the dataset\n",
    "    for (source_images, source_labels), (target_images, _) in zip(src_dataloader, target_dataloader):\n",
    "        source_images = source_images.to(DEVICE)\n",
    "        target_images = target_images.to(DEVICE)\n",
    "        source_labels = source_labels.to(DEVICE)\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute the classifier output for SOURCE images\n",
    "        classifier_source_outputs = DANN(source_images, True)\n",
    "        domain_classifier_source_outputs = DANN(source_images, False)\n",
    "        domain_classifier_target_outputs = DANN(target_images, False)\n",
    "\n",
    "        # compute the loss for the classifier\n",
    "        loss = criterion(classifier_source_outputs, source_labels) # supervised task\n",
    "        loss += criterion(domain_classifier_source_outputs, torch.full((BATCH_SIZE, ), 1, device=DEVICE))\n",
    "        loss += criterion(domain_classifier_source_outputs, torch.full((BATCH_SIZE, ), 0, device=DEVICE))\n",
    "\n",
    "        # compute and propagate the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    source_loss = epoch_loss / len(src_dataloader)\n",
    "    print(f\"[SOURCE]: (avg) loss is {source_loss:.3f}\")\n",
    "\n",
    "    target_loss = compute_classification_loss(target_dataloader)\n",
    "    target_accuracy = compute_accuracy(target_dataloader)\n",
    "    print(f\"[TARGET]: loss is {target_loss:.3f}, accuracy is {target_accuracy:.3f}\")\n",
    "    \n",
    "    # compute the average epoch loss\n",
    "    # wandb.log({\"training-loss\": training_loss})\n",
    "\n",
    "    # at the end of each epoch compute the accuracy on the validation accuracy\n",
    "    # wandb.log({\"validation-accuracy\": validation_loss})\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework2-MLDL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "54126dcf83d3b19f1df3b8c2eeabae9ee231726106b34df49b0e6acbb809dc7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
